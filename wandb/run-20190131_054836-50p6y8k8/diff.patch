diff --git a/.gitignore.txt b/.gitignore.txt
index 7815be8..fc8334b 100644
--- a/.gitignore.txt
+++ b/.gitignore.txt
@@ -1,2 +1,3 @@
 torque_estimation/wandb/
-torque_estimation/.vscode
\ No newline at end of file
+torque_estimation/.vscode
+wandb/
diff --git a/ee_force_estimation_7dof_nn.py b/ee_force_estimation_7dof_nn.py
index b6cee58..3032fb5 100644
--- a/ee_force_estimation_7dof_nn.py
+++ b/ee_force_estimation_7dof_nn.py
@@ -4,8 +4,13 @@ import csv
 import matplotlib.pyplot as plt
 from sklearn import preprocessing
 import time
+import wandb
+import os
 
+wandb_use = True
 start_time = time.time()
+if wandb_use == True:
+    wandb.init(project="dusan_ws", tensorboard=False)
 
 class Model:
 
@@ -25,39 +30,35 @@ class Model:
             # weights & bias for nn layers
             # http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow
             W1 = tf.get_variable("W1", shape=[num_input, 10], initializer=tf.contrib.layers.xavier_initializer())
-            b1 = tf.Variable(tf.random_normal([10]))
+            b1 = tf.Variable(tf.random_normal([10]), name='b1')
             L1 = tf.matmul(self.X, W1) +b1
-            #L1 = tf.layers.batch_normalization(L1, center=True, scale=True, training=1)
             L1 = tf.nn.relu(L1)
             #L1 = tf.nn.sigmoid(tf.matmul(self.X, W1) + b1)
             L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)
 
             W2 = tf.get_variable("W2", shape=[10, 10], initializer=tf.contrib.layers.xavier_initializer())
-            b2 = tf.Variable(tf.random_normal([10]))
+            b2 = tf.Variable(tf.random_normal([10]), name='b2')
             L2 = tf.matmul(L1, W2) +b2
-            #L2 = tf.layers.batch_normalization(L2, center=True, scale=True, training=1)
             L2 = tf.nn.relu(L2)
             #L2 = tf.nn.sigmoid(tf.matmul(L1, W2) + b2)
             L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)
 
             W3 = tf.get_variable("W3", shape=[10, 10], initializer=tf.contrib.layers.xavier_initializer())
-            b3 = tf.Variable(tf.random_normal([10]))
+            b3 = tf.Variable(tf.random_normal([10]), name='b3')
             L3 = tf.matmul(L2, W3) +b3
-            #L3 = tf.layers.batch_normalization(L3, center=True, scale=True, training=1)
             L3 = tf.nn.relu(L3)
             #L3 = tf.nn.relu(tf.sigmoid(L2, W3) + b3)
             L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)
 
             W4 = tf.get_variable("W4", shape=[10, 10], initializer=tf.contrib.layers.xavier_initializer())
-            b4 = tf.Variable(tf.random_normal([10]))
+            b4 = tf.Variable(tf.random_normal([10]), name='b4')
             L4 = tf.matmul(L3, W4) +b4
-            #L4 = tf.layers.batch_normalization(L4, center=True, scale=True, training=1)
             L4 = tf.nn.relu(L4)
             #L4 = tf.nn.relu(tf.sigmoid(L3, W4) + b4)
             L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)
 
-            W5 = tf.get_variable("W6", shape=[10, num_output], initializer=tf.contrib.layers.xavier_initializer())
-            b5 = tf.Variable(tf.random_normal([num_output]))
+            W5 = tf.get_variable("W5", shape=[10, num_output], initializer=tf.contrib.layers.xavier_initializer())
+            b5 = tf.Variable(tf.random_normal([num_output]), name='b5')
             self.hypothesis = tf.matmul(L4, W5) + b5
             self.hypothesis = tf.identity(self.hypothesis, "hypothesis")
 
@@ -82,8 +83,8 @@ class Model:
         for line in data:
             line = [float(i) for i in line]
             x_batch.append(line[1:num_input+1])
-            y_batch.append(line[-num_output:])
-            #y_batch.append(line[-output_idx])
+            #y_batch.append(line[-num_output:])
+            y_batch.append(line[-output_idx])
             i = i+1
 
             if i == num:
@@ -104,12 +105,11 @@ for line in rdr:
     line = [float(i) for i in line]
     t.append(line[0])
     x_data_test.append(line[1:num_input+1])
-    y_data_test.append(line[-num_output:])
-    #y_data_test.append(line[-output_idx])
+    #y_data_test.append(line[-num_output:])
+    y_data_test.append(line[-output_idx])
 
 t = np.reshape(t,(-1,1))
 x_data_test = np.reshape(x_data_test, (-1, num_input))
-#x_data_test = preprocessing.scale(x_data_test)
 y_data_test = np.reshape(y_data_test, (-1, num_output))
 
 # load validation data
@@ -120,10 +120,9 @@ y_data_val = []
 for line in rdr:
     line = [float(i) for i in line]
     x_data_val.append(line[1:num_input+1])
-    y_data_val.append(line[-num_output:])
-    #y_data_val.append(line[-output_idx])
+    #y_data_val.append(line[-num_output:])
+    y_data_val.append(line[-output_idx])
 x_data_val = np.reshape(x_data_val, (-1, num_input))
-#x_data_val = preprocessing.scale(x_data_val)
 y_data_val = np.reshape(y_data_val, (-1, num_output))
 
 # parameters
@@ -133,7 +132,13 @@ batch_size = 100
 total_batch = int(np.shape(x_data_test)[0]/batch_size*5)
 drop_out = 1.0
 
-
+if wandb_use == True:
+    wandb.config.epoch = training_epochs
+    wandb.config.batch_size = batch_size
+    wandb.config.learning_rate = learning_rate
+    wandb.config.drop_out = drop_out
+    wandb.config.num_input = num_input
+    wandb.config.num_output = num_output
 
 # initialize
 sess = tf.Session()
@@ -151,7 +156,6 @@ for epoch in range(training_epochs):
 
     for i in range(total_batch):
         batch_xs, batch_ys = m1.next_batch(batch_size, rdr)
-        #batch_xs = preprocessing.scale(batch_xs)
         c, _ = m1.train(batch_xs, batch_ys, drop_out)
         avg_cost += c / total_batch
 
@@ -163,6 +167,14 @@ for epoch in range(training_epochs):
     train_mse[epoch] = avg_cost
     validation_mse[epoch] = cost
 
+    if wandb_use == True:
+        wandb.log({'training cost': avg_cost, 'validation cost': cost})
+
+        if epoch % 20 ==0:
+            for var in tf.trainable_variables():
+                name = var.name
+                wandb.log({name: sess.run(var)})
+
 print('Learning Finished!')
 
 
@@ -170,12 +182,18 @@ print('Learning Finished!')
 # print('Error: ', error,"\n x_data: ", x_test,"\nHypothesis: ", hypo, "\n y_data: ", y_test)
 print('Test Error: ', error)
 
-saver = tf.train.Saver()
-saver.save(sess,'model/model.ckpt')
 
 elapsed_time = time.time() - start_time
 print(elapsed_time)
 
+
+saver = tf.train.Saver()
+saver.save(sess,'model/model.ckpt')
+
+if wandb_use == True:
+    wandb.save(os.path.join(wandb.run.dir, 'model/model.ckpt'))
+    wandb.config.elapsed_time = elapsed_time
+
 epoch = np.arange(training_epochs)
 plt.plot(epoch, train_mse, 'r', label='train')
 plt.plot(epoch, validation_mse, 'b', label='validation')
diff --git a/model/model.ckpt.data-00000-of-00001 b/model/model.ckpt.data-00000-of-00001
index b0e4368..e37011f 100644
Binary files a/model/model.ckpt.data-00000-of-00001 and b/model/model.ckpt.data-00000-of-00001 differ
diff --git a/model/model.ckpt.index b/model/model.ckpt.index
index fd32bbe..80cf2d5 100644
Binary files a/model/model.ckpt.index and b/model/model.ckpt.index differ
diff --git a/model/model.ckpt.meta b/model/model.ckpt.meta
index c726684..1ca3c59 100644
Binary files a/model/model.ckpt.meta and b/model/model.ckpt.meta differ
